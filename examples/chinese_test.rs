//! ä¸­æ–‡æ–‡æœ¬å¤„ç†æµ‹è¯• - æ— éœ€ API è°ƒç”¨
//!
//! æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•æµ‹è¯•ä¸­æ–‡æ–‡æœ¬çš„åˆ†è¯å’Œè§£æåŠŸèƒ½ï¼Œæ— éœ€å®é™…è°ƒç”¨ LLM APIã€‚
//!
//! è¿è¡Œæ­¤ç¤ºä¾‹: cargo run --example chinese_test

use langextract::{resolver::Resolver, tokenizer::tokenize};

fn main() {
    println!("ğŸ® ä¸­æ–‡æ–‡æœ¬å¤„ç†æµ‹è¯•");
    println!("===================\n");

    // æµ‹è¯• 1: ä¸­æ–‡åˆ†è¯
    println!("ğŸ“ æµ‹è¯• 1: ä¸­æ–‡åˆ†è¯åŠŸèƒ½");
    let chinese_text = "å®ç‰ä»Šæ—¥ç©¿äº†ä¸€ä»¶æœˆç™½ç¼å­è¢å­ï¼Œè…°ç³»ä¸ç»¦ï¼Œå¤´æˆ´ç´«é‡‘å† ï¼Œè„šè¹¬äº‘å¤´å±¥ã€‚";
    println!("è¾“å…¥æ–‡æœ¬: {}", chinese_text);

    let tokenized = tokenize(chinese_text);
    println!("åˆ†è¯ç»“æœ: {} ä¸ª token", tokenized.tokens.len());

    for (i, token) in tokenized.tokens.iter().enumerate().take(10) {
        let token_text = &chinese_text[token.char_interval.start_pos..token.char_interval.end_pos];
        println!("  {}. ã€Œ{}ã€ - {:?}", i + 1, token_text, token.token_type);
    }

    if tokenized.tokens.len() > 10 {
        println!("  ... è¿˜æœ‰ {} ä¸ª token", tokenized.tokens.len() - 10);
    }

    // æµ‹è¯• 2: è§£æå™¨å¤„ç†åµŒå¥— YAML æ ¼å¼
    println!("\nğŸ“ æµ‹è¯• 2: è§£æå™¨å¤„ç†ä¸­æ–‡ YAML");
    let resolver = Resolver::new(true, None, None, true);

    let mock_yaml_response = r#"```yaml
characters:
  - å®ç‰
  - è¢­äºº
  - æ—å§‘å¨˜
  - é»›ç‰
locations:
  - æ€¡çº¢é™¢
  - æ½‡æ¹˜é¦†
objects:
  - æœˆç™½ç¼å­è¢å­
  - ä¸ç»¦
  - ç´«é‡‘å† 
  - äº‘å¤´å±¥
  - ç¢§èºæ˜¥èŒ¶
emotions:
  - å¿ƒä¸­æ¬¢å–œ
  - å¿ƒæƒ…ç”šæ˜¯æ„‰æ‚¦
  - ç¥æƒ…å“€æ€¨
```"#;

    println!("æ¨¡æ‹Ÿ LLM å“åº”:");
    println!("{}", mock_yaml_response);

    match resolver.parse_extractions_from_string(mock_yaml_response) {
        Ok(extractions) => {
            println!("\nâœ… æˆåŠŸè§£æ {} ä¸ªæå–é¡¹!", extractions.len());

            // æŒ‰ç±»åˆ«åˆ†ç»„
            use std::collections::HashMap;
            let mut categories: HashMap<String, Vec<&str>> = HashMap::new();

            for extraction in &extractions {
                let category = if extraction.extraction_class.contains("characters") {
                    "äººç‰©"
                } else if extraction.extraction_class.contains("locations") {
                    "åœ°ç‚¹"
                } else if extraction.extraction_class.contains("objects") {
                    "ç‰©å“"
                } else if extraction.extraction_class.contains("emotions") {
                    "æƒ…æ„Ÿ"
                } else {
                    "å…¶ä»–"
                };

                categories
                    .entry(category.to_string())
                    .or_insert_with(Vec::new)
                    .push(&extraction.extraction_text);
            }

            for (category, items) in categories {
                println!("\nğŸ“‹ {}: ({} é¡¹)", category, items.len());
                for (i, item) in items.iter().enumerate() {
                    println!("  {}. ã€Œ{}ã€", i + 1, item);
                }
            }
        }
        Err(e) => {
            println!("âŒ è§£æå¤±è´¥: {:?}", e);
        }
    }

    // æµ‹è¯• 3: ä¸­è‹±æ··åˆæ–‡æœ¬
    println!("\nğŸ“ æµ‹è¯• 3: ä¸­è‹±æ··åˆæ–‡æœ¬åˆ†è¯");
    let mixed_text = "Helloä¸–ç•Œ! This isä¸€ä¸ªæµ‹è¯•example.";
    println!("æ··åˆæ–‡æœ¬: {}", mixed_text);

    let mixed_tokenized = tokenize(mixed_text);
    println!("åˆ†è¯ç»“æœ:");
    for (i, token) in mixed_tokenized.tokens.iter().enumerate() {
        let token_text = &mixed_text[token.char_interval.start_pos..token.char_interval.end_pos];
        println!("  {}. ã€Œ{}ã€ - {:?}", i + 1, token_text, token.token_type);
    }

    println!("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼");
    println!("âœ… ä¸­æ–‡åˆ†è¯: æ­£å¸¸å·¥ä½œ");
    println!("âœ… åµŒå¥— YAML è§£æ: æ­£å¸¸å·¥ä½œ");
    println!("âœ… ä¸­è‹±æ··åˆ: æ­£å¸¸å·¥ä½œ");
    println!("\nğŸ’¡ ç°åœ¨æ‚¨å¯ä»¥æ”¾å¿ƒä½¿ç”¨ä¸­æ–‡æ–‡æœ¬è¿›è¡Œå®é™…çš„ LangExtract æ“ä½œäº†ï¼");
}
